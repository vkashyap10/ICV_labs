{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Dataset/DatasetA/'\n",
    "\n",
    "# create directory to store results\n",
    "!mkdir Results\n",
    "!mkdir Results/Q4\n",
    "!mkdir Results/Q4/partA\n",
    "\n",
    "pathA = \"Results/Q4/partA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841996df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of images\n",
    "import os\n",
    "jpg_files = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for filename in files:\n",
    "        if \"jpg\" in filename:\n",
    "            jpg_files.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08b767",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99e670be",
   "metadata": {},
   "source": [
    "There are 3 ways in which we could deal with borders in window:\n",
    "compute LBP for window without the border\n",
    "compute LBP for window by taking mirror image\n",
    "compute LBP for window by padding with zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8805ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that divides a greyscale image into equally sized non-overlapping windows and returns the \n",
    "# feature descriptor for each window as distribution of LBP codes.\n",
    "\n",
    "# input : image : image to take LBP of \n",
    "#         window_width : width of window to divide full image\n",
    "#         window_length : length of window to divide full image\n",
    "#.        border : how do we want to handel the border : 3 option , ignore border, mirror or pad with zeros\n",
    "#         plot : boolean to plot image and save\n",
    "\n",
    "# return \n",
    "\n",
    "def ICV_LBP(image,window_length,window_width,border_case,plot):\n",
    "    \n",
    "    # to store lbp of image\n",
    "    lbp_full_image = np.zeros(image.shape)\n",
    "    \n",
    "    # to store lbps based on windows\n",
    "    lbp_window = []\n",
    "    \n",
    "    # this will implement border_case == 'ignore_boreder' by starting and ending for loop accordingly\n",
    "    ignore_int = 0\n",
    "    \n",
    "    # new image created for implementing different border halding methods\n",
    "    new_image = np.zeros((image.shape[0]+2,image.shape[1]+2))\n",
    "    new_image[1:image.shape[0]+1,1:image.shape[1]+1] = image.copy()\n",
    "    \n",
    "    # we have to take care of borders. We can either put zeros or take mirror image.\n",
    "    # 3 cases of how to deal with borders\n",
    "    if border_case == 'ignore_border':\n",
    "        ignore_int = 1\n",
    "        \n",
    "    if border_case == 'mirror':\n",
    "        # pad image with first and last row and columns\n",
    "        new_image[0,1:image.shape[1]+1] = image[0,:]\n",
    "        new_image[-1,1:image.shape[1]+1] = image[-1,:]\n",
    "        new_image[1:image.shape[0]+1,0] = image[:,0]\n",
    "        new_image[1:image.shape[0]+1,-1] = image[:,-1]\n",
    "        \n",
    "    if border_case == 'zeros':\n",
    "        # pad image with first and last row and columns\n",
    "        # already padded with zeros\n",
    "        ignore_int = 0\n",
    "        \n",
    "    # consider a single window in every iteration of i,j loop\n",
    "    for i in range(1,1+image.shape[0],window_width):\n",
    "        for j in range(1,1+image.shape[1],window_length):\n",
    "            \n",
    "            # to store local binary descriptor for each pixel\n",
    "            lbp_freq = np.zeros((256))\n",
    "            # working on window in k,l loop\n",
    "            for k in range(i+ignore_int,min(i+window_width,image.shape[0])-ignore_int):\n",
    "                for l in range(j+ignore_int,min(j+window_length,image.shape[1])-ignore_int):\n",
    "                    \n",
    "                    # get bits and convert to decimal\n",
    "                    bit_1 = new_image[k-1,l-1] > new_image[k,l]\n",
    "                    bit_2 = new_image[k-1,l] > new_image[k,l]\n",
    "                    bit_3 = new_image[k-1,l+1]>new_image[k,l]\n",
    "                    bit_4 = new_image[k,l+1] > new_image[k,l]\n",
    "                    bit_5 = new_image[k+1,l+1] > new_image[k,l]\n",
    "                    bit_6 = new_image[k+1,l]> new_image[k,l]\n",
    "                    bit_7 = new_image[k+1,l-1]> new_image[k,l]\n",
    "                    bit_8 = new_image[k,l-1]> new_image[k,l]\n",
    "\n",
    "                    decimal_number = bit_1*np.power(2,7) + bit_2*np.power(2,6) + bit_3*np.power(2,5)+ bit_4*np.power(2,4)+ bit_5*np.power(2,3)+ bit_6*np.power(2,2)+ bit_7*np.power(2,1)+ bit_8*np.power(2,0)\n",
    "                    \n",
    "                    # store lbp value for classification and printing later\n",
    "                    lbp_full_image[k-1,l-1] = decimal_number\n",
    "                    lbp_freq[decimal_number] = lbp_freq[decimal_number] + 1\n",
    "            \n",
    "            \n",
    "            if(plot == True):\n",
    "                # window image\n",
    "                fig = plt.figure()\n",
    "                sidx_x = i+ignore_int-1\n",
    "                sidx_y = j+ignore_int-1\n",
    "                eidx_x = min(i+window_width,image.shape[0])-ignore_int -1\n",
    "                eidx_y = min(j+window_length,image.shape[1])-ignore_int - 1\n",
    "                plt.imshow(image[sidx_x:eidx_x,sidx_y:eidx_y],cmap='gray')\n",
    "                plt.title(\"image window for row and columns, \"+str(i-1)+\" , \"+str(j-1))\n",
    "                fig.savefig(pathA+\"image_Frame_\"+str(i-1)+\" , \"+str(j-1)+\".png\")\n",
    "                plt.show()\n",
    "\n",
    "                # lbp of window image\n",
    "                fig = plt.figure()\n",
    "                sidx_x = i+ignore_int-1\n",
    "                sidx_y = j+ignore_int-1\n",
    "                eidx_x = min(i+window_width,image.shape[0])-ignore_int -1\n",
    "                eidx_y = min(j+window_length,image.shape[1])-ignore_int - 1\n",
    "                # store window lbp \n",
    "                lbp_window.append(lbp_full_image[sidx_x:eidx_x,sidx_y:eidx_y])\n",
    "                plt.imshow(lbp_full_image[sidx_x:eidx_x,sidx_y:eidx_y],cmap='gray')\n",
    "                plt.title(\"LBP for window row and columns, \"+str(i-1)+\" , \"+str(j-1))\n",
    "                fig.savefig(pathA+\"LBP_Frame_\"+str(i-1)+\" , \"+str(j-1)+\".png\")\n",
    "                plt.show()\n",
    "\n",
    "                # normalize histogram\n",
    "                lbp_freq = lbp_freq/np.sum(lbp_freq)\n",
    "                # use pyplot.bar to make histogram\n",
    "                fig = plt.figure()\n",
    "                plt.bar(np.arange(lbp_freq.shape[0]),lbp_freq)\n",
    "                plt.xlabel(\"LBP number in decimal\")\n",
    "                plt.ylabel(\"Number of pixels in bin\")\n",
    "                plt.title(\"LBP histogram for window row and columns, \"+str(i-1)+\" , \"+str(j-1))\n",
    "                fig.savefig(pathA+\"Histogram_\"+str(i-1)+\" , \"+str(j-1)+\".png\")\n",
    "                plt.show()\n",
    "            \n",
    "    return lbp_full_image,lbp_window\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3d630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load image to find LBP\n",
    "figure(figsize=(14, 11))\n",
    "\n",
    "# can change index from 5 to any number from 0 to 5 to get lbp of that image.\n",
    "\n",
    "img = cv2.imread(data_path+jpg_files[5])\n",
    "\n",
    "# take grayscale of image\n",
    "img = img[:,:,0]*1/3 + img[:,:,1]*1/3 + img[:,:,2]*1/3\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aad6d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get lbp of full image and windows using a window size of 128x128\n",
    "lbp_full_image,lbp_window = ICV_LBP(img,128,128,\"mirror\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4db20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lbp of full image\n",
    "figure(figsize=(14, 11))\n",
    "plt.imshow(lbp_full_image,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0847f85",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d539cc7",
   "metadata": {},
   "source": [
    " b) Come up with a descriptor that represents the whole image as consisting of multiple windows. For example, you could combine several local descriptions into a global description by concatenation. Discuss in the report alternative approaches. Using the global descriptor you created, implement a classification process that separates the images in the dataset into two categories: face images and non-face images (for example, you could use histogram similarities). Comment the results in the report. Is the global descriptor able to represent whole images of different types (e.g. faces vs. cars)? Identify problems (if any), discuss them in the report and suggest possible solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d12a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in lbp of full image and returns an array of lbps of all windows in an image\n",
    "# input : image : LBP of full image\n",
    "#         window_lenght : window length in which to divide full image lbp\n",
    "#         window_width : window width in which to divide full image lbp\n",
    "# return : array of lbps of windows\n",
    "def ICV_lbp_descriptor(image,window_length,window_width):\n",
    "    \n",
    "    # create the full image lbp using mirror adjustment. for borders\n",
    "    lbp_full_image,lbp_window = ICV_LBP(image,window_length,window_width,\"mirror\",False)\n",
    "    \n",
    "    # to store reshaped lbp of every window\n",
    "    window_lbps = []\n",
    "    # loop through the full image lbp\n",
    "    for i in range(0,image.shape[0],window_width):\n",
    "        for j in range(0,image.shape[1],window_length):\n",
    "            \n",
    "            # get lbp of window and store it\n",
    "            lbp_window = lbp_full_image[i:i+window_width,j:j+window_length]\n",
    "            window_lbps.append(lbp_window.reshape(-1,1))\n",
    "            \n",
    "    return np.squeeze(np.array(window_lbps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ee184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through each image and get lbp descriptors using ICV_lbp_descriptor\n",
    "def ICV_all_descriptor(filenames,window_length,window_width):\n",
    "    global_lbp_descriptors = []\n",
    "    for i in range(len(filenames)):\n",
    "        print(\"working on image \",i)\n",
    "        img = cv2.imread(data_path+filenames[i])\n",
    "        # as opencv loads in BGR format by default, we want to show it in RGB.\n",
    "        img = img[:,:,0]*1/3 + img[:,:,1]*1/3 +img[:,:,2]*1/3 \n",
    "        window_lbps = ICV_lbp_descriptor(img,window_length,window_width)\n",
    "        global_lbp_descriptors.append(window_lbps)\n",
    "    return global_lbp_descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0462adc",
   "metadata": {},
   "source": [
    "###  Plotting concatenated global descriptors, this will not be used for classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all descriptors for all images\n",
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,128,128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(jpg_files)):\n",
    "    print(\"working on image \",i)\n",
    "    img = cv2.imread(data_path +jpg_files[i])\n",
    "    img = img[:,:,0]*1/3 + img[:,:,1]*1/3 + img[:,:,2]*1/3\n",
    "    print(jpg_files[i])\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    glbp = all_lbp_descriptors[i]\n",
    "    \n",
    "    # this will store concatenated lbp\n",
    "    lbp_freq = np.zeros((glbp.shape[0],256))\n",
    "    # concatenate lbps from all windows\n",
    "    for j in range(glbp.shape[0]):\n",
    "        for k in range(glbp.shape[1]):\n",
    "            lbp_freq[j,int(glbp[j][k])] = lbp_freq[j,int(glbp[j][k])] + 1\n",
    "    \n",
    "    lbp_freq = lbp_freq/np.sum(lbp_freq,1)[:,np.newaxis]\n",
    "    # concatenate lbp\n",
    "    lbp_freq = np.squeeze(lbp_freq.reshape(-1,1))\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=80)\n",
    "    plt.bar(np.arange(lbp_freq.shape[0]),lbp_freq)\n",
    "    plt.xlabel(\"LBP number in decimal\")\n",
    "    plt.ylabel(\"Number of pixels in bin\")\n",
    "    plt.title(\"LBP histogram for full image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8631ee0a",
   "metadata": {},
   "source": [
    "##### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf300ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given all local descriptors, compute the similarity between corresponding window lbps in all images.\n",
    "# then similarity between two images is defines as the mean of corresponding window similarities\n",
    "# Using this we can get similarity between all images to get a matrix num_images x num_images\n",
    "# Apply k-means on this similarity matrix to get labels.\n",
    "\n",
    "# input : all_lbp_descriptors : list of window lbps for all images\n",
    "#         filenames : names of images \n",
    "#         return : similarity datafram\n",
    "\n",
    "def ICV_lbp_classify(all_lbp_descriptors,filenames):\n",
    "    \n",
    "    # find out count of lbp for each window in every image. This will help in calculating histogram similarity\n",
    "    lbp_count_all_image = []\n",
    "    for i in range(len(all_lbp_descriptors)):\n",
    "        lbp_count_image = np.zeros((all_lbp_descriptors[i].shape[0],256))\n",
    "        for j in range(all_lbp_descriptors[i].shape[0]):\n",
    "            temp_descriptor = all_lbp_descriptors[i][j,:]\n",
    "            for k in range(temp_descriptor.shape[0]):\n",
    "                lbp_count_image[j,int(temp_descriptor[k])] = lbp_count_image[j,int(temp_descriptor[k])] + 1\n",
    "        lbp_count_image = lbp_count_image/np.sum(lbp_count_image,1)[:,np.newaxis]\n",
    "        lbp_count_all_image.append(lbp_count_image)\n",
    "        \n",
    "    # compute histogram similarity of one image with every other image. Take mean of histogram similarity across window.\n",
    "\n",
    "    similarity_matrix = np.zeros((len(filenames),len(filenames)))\n",
    "    for i in range(len(lbp_count_all_image)):\n",
    "        for j in range(len(lbp_count_all_image)):\n",
    "            hist_similarity = 2*np.sum(np.minimum(lbp_count_all_image[i],lbp_count_all_image[j]),1)/(np.sum(lbp_count_all_image[i],1)+np.sum(lbp_count_all_image[j],1))\n",
    "            mean_hist_similarity = np.mean(hist_similarity)\n",
    "            similarity_matrix[i,j] = mean_hist_similarity\n",
    "\n",
    "        \n",
    "    similarity_df = pd.DataFrame(similarity_matrix, columns = jpg_files, index=jpg_files)\n",
    "    display(similarity_df)\n",
    "    \n",
    "    # use k means to give labels\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(similarity_matrix)\n",
    "    print(\"labels for images\",kmeans.labels_)\n",
    "    \n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6930b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,128,128)\n",
    "df_128 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c041115",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643d0da",
   "metadata": {},
   "source": [
    "c) Decrease the window size and perform classification again. Comment the results in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the widow size from 128X128 to 4x4,8x8,16x16, 632x32 and 64x64. Store results to analyse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,4,4)\n",
    "df_4 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e225781",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,8,8)\n",
    "df_8 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,16,16)\n",
    "df_16 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270648b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,32,32)\n",
    "df_32 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e70024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,64,64)\n",
    "df_64 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37adce",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89c8c4",
   "metadata": {},
   "source": [
    "d) Increase the window size and perform classification again. Comment the results in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c255f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing window size from 128X128 to 256X256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d508b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_lbp_descriptors = ICV_all_descriptor(jpg_files,256,256)\n",
    "for i in range(len(all_lbp_descriptors)):\n",
    "    all_lbp_descriptors[i] = all_lbp_descriptors[i][:,np.newaxis].T\n",
    "df_256 = ICV_lbp_classify(all_lbp_descriptors,jpg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f6dd4",
   "metadata": {},
   "source": [
    "## Analysing effect of window size on classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3946bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean of all car and face images and compare with means of off diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_mean_256 = (np.sum(np.array(df_256)[3:,3:])-3)/6\n",
    "cars_mean_128 = (np.sum(np.array(df_128)[3:,3:])-3)/6\n",
    "cars_mean_64 = (np.sum(np.array(df_64)[3:,3:])-3)/6\n",
    "cars_mean_32 = (np.sum(np.array(df_32)[3:,3:])-3)/6\n",
    "cars_mean_16 = (np.sum(np.array(df_16)[3:,3:])-3)/6\n",
    "cars_mean_8 = (np.sum(np.array(df_8)[3:,3:])-3)/6\n",
    "# cars_mean_2 = (np.sum(np.array(df_2)[3:,3:])-3)/6\n",
    "\n",
    "face_mean_256 = (np.sum(np.array(df_256)[:3,:3])-3)/6\n",
    "face_mean_128 = (np.sum(np.array(df_128)[:3,:3])-3)/6\n",
    "face_mean_64 = (np.sum(np.array(df_64)[:3,:3])-3)/6\n",
    "face_mean_32 = (np.sum(np.array(df_32)[:3,:3])-3)/6\n",
    "face_mean_16 = (np.sum(np.array(df_16)[:3,:3])-3)/6\n",
    "face_mean_8 = (np.sum(np.array(df_8)[:3,:3])-3)/6\n",
    "# face_mean_2 = (np.sum(np.array(df_2)[:3,:3])-3)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# off diagonal mean\n",
    "off_mean_256 = np.mean(np.array(df_256)[3:,:3]+np.array(df_256)[:3,3:])/2\n",
    "off_mean_128 = np.mean(np.array(df_128)[3:,:3]+np.array(df_128)[:3,3:])/2\n",
    "off_mean_64 = np.mean(np.array(df_64)[3:,:3]+np.array(df_64)[:3,3:])/2\n",
    "off_mean_32 = np.mean(np.array(df_32)[3:,:3]+np.array(df_32)[:3,3:])/2\n",
    "off_mean_16 = np.mean(np.array(df_16)[3:,:3]+np.array(df_16)[:3,3:])/2\n",
    "off_mean_8 = np.mean(np.array(df_8)[3:,:3]+np.array(df_8)[:3,3:])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88549244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "x_labels = ['8x8','16x16','32x32','64x64','128x128','256x256']\n",
    "plt.plot(x_labels,[cars_mean_8,cars_mean_16,cars_mean_32,cars_mean_64,cars_mean_128,cars_mean_256],label='mean of similarity between cars')\n",
    "plt.plot(x_labels,[face_mean_8,face_mean_16,face_mean_32,face_mean_64,face_mean_128,face_mean_256],label='mean of similarity between faces')\n",
    "plt.plot(x_labels,[off_mean_8,off_mean_16,off_mean_32,off_mean_64,off_mean_128,off_mean_256],label='mean of similarity between heterogeneous labels')\n",
    "plt.legend()\n",
    "plt.title(\"effect of size of classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53534b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "x_labels = ['8x8','16x16','32x32','64x64','128x128','256x256']\n",
    "car_mean_array = np.array([cars_mean_8,cars_mean_16,cars_mean_32,cars_mean_64,cars_mean_128,cars_mean_256])\n",
    "face_mean_array = np.array([face_mean_8,face_mean_16,face_mean_32,face_mean_64,face_mean_128,face_mean_256])\n",
    "off_mean_array = np.array([off_mean_8,off_mean_16,off_mean_32,off_mean_64,off_mean_128,off_mean_256])\n",
    "\n",
    "spread1 = car_mean_array - off_mean_array\n",
    "spread2 = face_mean_array - off_mean_array\n",
    "\n",
    "plt.plot(x_labels,spread1,label='spread 1 : car_mean - heterogeneous_mean')\n",
    "plt.plot(x_labels,spread2,label='spread 2 : face_mean - heterogeneous_mean')\n",
    "plt.legend()\n",
    "plt.title(\"spread between same type images and different type images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9066f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
